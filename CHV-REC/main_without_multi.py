import torch.optim as optim
import sys
import math
from Models_multi_task import *
from utility.helper import ensureDir
from utility.batch_test import data_generator
import h5py
from scipy.spatial.distance import cosine
import numpy as np
import json
import torch
import warnings
import random
import torch.nn.functional as F
import os
import scipy.sparse as sp
from time import time
import torch.sparse as sparse
from utility.parser import parse_args
args = parse_args()
warnings.filterwarnings("ignore")



def lamb(epoch):
    epoch += 0
    return 0.95 ** (epoch / 14)

result = []
alpha1=args.alpha1
alpha2=args.alpha2
alpha3=args.alpha3
alpha4=args.alpha4

seed = args.seed
random.seed(seed)
np.random.seed(seed)
torch.manual_seed(seed)
torch.cuda.manual_seed_all(seed)



def jaccard_similarity(matrix):
    intersection = np.dot(matrix, matrix.T)
    square_sum = np.diag(intersection)  # è·å–å¯¹è§’çº¿ä¸Šçš„å…ƒç´ 
    union = square_sum[:, None] + square_sum - intersection
    return np.divide(intersection, union)


class Model_Wrapper(object):
    def __init__(self, data_config, pretrain_data):
        # argument settings
        self.model_type = args.model_type
        self.adj_type = args.adj_type
        self.alg_type = args.alg_type
        self.mess_dropout = eval(args.mess_dropout)
        self.pretrain_data = pretrain_data
        self.n_mashup = data_config['n_users']
        self.n_api = data_config['n_items']
        self.n_tag=data_config['n_tags']

        self.record_alphas = False
        self.lr = args.lr
        self.emb_dim = args.embed_size
        self.batch_size = args.batch_size
        self.layer_num = args.layer_num
        self.model_type += '_%s_%s_layers%d' % (self.adj_type, self.alg_type, self.layer_num)
        self.regs = eval(args.regs)
        self.decay = self.regs[0]
        self.verbose = args.verbose

        print('model_type is {}'.format(self.model_type))

        method = 'Without_multi'

        alpha_str = f"{args.alpha1}-{args.alpha2}-{args.alpha3}-{args.alpha4}"
        reg_str = '-'.join(str(r) for r in eval(args.regs))

        self.weights_save_path = (
            f"{args.weights_path}weights/{method}/{args.dataset}/{args.seed}/"
            f"alpha-{alpha_str}/"
            # "github"
            f"layer_num{args.layer_num}/"
            f"mess_dropout{args.mess_dropout}/"
            f"drop_edge{args.drop_edge}/"
            f"lr{args.lr}/"
            f"reg{reg_str}"
        )

        str(args.similar_threthold) + str(args.alpha1) + str(args.alpha2) + str(args.alpha3) + str(args.alpha4)
        self.result_message = []

        print('----self.alg_type is {}----'.format(self.alg_type))

        if self.alg_type in ['hcf']:
            self.model = HCF(self.n_mashup, self.n_api,self.n_tag, self.emb_dim, self.layer_num, self.mess_dropout)
        else:
            raise Exception('Dont know which model to train')

        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)

        # norm_mashup_call_1, norm_mashup_call_2,
        # norm_api_call_1, norm_api_call_2,
        # norm_tag_mashup_1, norm_tag_mashup_2,
        # norm_tag_api_1, norm_tag_api_2,
        # norm_mashup_tag_1, norm_mashup_tag_2,
        # norm_api_tag_1, norm_api_tag_2
        (self.norm_mashup_call_1, self.norm_mashup_call_2, self.norm_api_call_1, self.norm_api_call_2,
         self.norm_mashup_tag_1, self.norm_mashup_tag_2, self.norm_api_tag_1, self.norm_api_tag_2) = self.build_hyper_edge(
            args.data_path + args.dataset + '/TE.csv',args.data_path + args.dataset +'/mashup_tags.csv',args.data_path + args.dataset +'/api_tags.csv')
        print(f"norm_mashup_call_1 shape: {self.norm_mashup_call_1.shape}, norm_mashup_call_2 shape: {self.norm_mashup_call_2.shape}")
        self.global_1, self.global_2 = self.build_global_hyperedge(
            args.data_path + args.dataset + '/TE.csv', args.data_path + args.dataset + '/mashup_tags.csv',
            args.data_path + args.dataset + '/api_tags.csv', self.n_mashup, self.n_api, self.n_tag)
        # print(f"global_1 shape: {self.global_1.shape}, global_2 shape: {self.global_2.shape}")
        # print(f'n_mashup {self.n_mashup}')
        # print(f'n_api {self.n_api}')
        # print(f'n_tag {self.n_tag}')

        self.mashup_tag_labels = self.load_tag_labels(
            args.data_path + args.dataset + '/mashup_tags.csv',
            self.n_mashup, self.n_tag
        )
        self.api_tag_labels = self.load_tag_labels(
            args.data_path + args.dataset + '/api_tags.csv',
            self.n_api, self.n_tag
        )
        print(f"mashup_tag_labels: {self.mashup_tag_labels.shape}, api_tag_labels: {self.api_tag_labels.shape}")

        self.model = self.model.cuda()
        self.norm_mashup_call_1 = self.norm_mashup_call_1.cuda()
        self.norm_mashup_call_2 = self.norm_mashup_call_2.cuda()
        self.norm_api_call_1 = self.norm_api_call_1.cuda()
        self.norm_api_call_2 = self.norm_api_call_2.cuda()
        self.norm_mashup_tag_1 = self.norm_mashup_tag_1.cuda()
        self.norm_mashup_tag_2 = self.norm_mashup_tag_2.cuda()
        self.norm_api_tag_1 = self.norm_api_tag_1.cuda()
        self.norm_api_tag_2 = self.norm_api_tag_2.cuda()
        self.global_1 = self.global_1.cuda()
        self.global_2 = self.global_2.cuda()

        self.lr_scheduler = self.set_lr_scheduler()

    def get_D_inv(self, Hadj):

        H = sp.coo_matrix(Hadj.shape)
        H.row = Hadj.row.copy()
        H.col = Hadj.col.copy()
        H.data = Hadj.data.copy()
        rowsum = np.array(H.sum(1))
        columnsum = np.array(H.sum(0))

        Dv_inv = np.power(rowsum, -1).flatten()
        De_inv = np.power(columnsum, -1).flatten()
        Dv_inv[np.isinf(Dv_inv)] = 0.
        De_inv[np.isinf(De_inv)] = 0.

        Dv_mat_inv = sp.diags(Dv_inv)
        De_mat_inv = sp.diags(De_inv)
        return Dv_mat_inv, De_mat_inv

    def build_hyper_edge(self, mashup_api_call_file, mashup_tag_call_file, api_tag_call_file):
        def read_and_fill_matrix(filename, shape, transpose=False):
            matrix = np.zeros(shape)
            with open(filename) as f:
                for line in f:
                    line = line.strip()
                    if not line:
                        continue
                    parts = [item for item in line.split(" ") if item != '']
                    main_id = int(parts[0])
                    related_ids = list(map(int, parts[1:]))
                    if transpose:
                        matrix[related_ids, main_id] = 1
                    else:
                        matrix[main_id, related_ids] = 1
            return matrix

        def build_and_normalize_hyperedge(interaction_matrix, similarity_threshold, node_count, edge_prefix):
            jaccard_sim = jaccard_similarity(interaction_matrix)
            indices = np.where(jaccard_sim >= similarity_threshold)
            values = jaccard_sim[indices]
            print(f"Number of hyperedges for {edge_prefix}: {len(values)}")

            hyperedge = sp.coo_matrix((values, indices), (node_count, node_count))
            hyperedge_transpose = hyperedge.T

            Dv, De = self.get_D_inv(hyperedge_transpose)

            Dv_torch = self.sparse_mx_to_torch_sparse_tensor(Dv)
            De_torch = self.sparse_mx_to_torch_sparse_tensor(De)
            hyperedge_torch = self.sparse_mx_to_torch_sparse_tensor(hyperedge_transpose)
            hyperedge_transpose_torch = self.sparse_mx_to_torch_sparse_tensor(hyperedge)

            temp = sparse.mm(Dv_torch, hyperedge_torch)
            normalized_1 = sparse.mm(temp, De_torch)
            normalized_2 = hyperedge_transpose_torch

            setattr(self, f'H{edge_prefix}Edge', hyperedge_torch)
            setattr(self, f'H{edge_prefix}Edge_T', hyperedge_transpose_torch)
            setattr(self, f'norm_{edge_prefix}1', normalized_1)
            setattr(self, f'norm_{edge_prefix}2', normalized_2)

            return normalized_1, normalized_2

        # mashupè°ƒç”¨APIäº¤äº’çŸ©é˜µ
        mashup_api_call_matrix = read_and_fill_matrix(mashup_api_call_file, (self.n_mashup, self.n_api), transpose=False)
        # APIè°ƒç”¨mashupçŸ©é˜µï¼ˆè½¬ç½®ï¼‰
        api_mashup_matrix = read_and_fill_matrix(mashup_api_call_file, (self.n_api, self.n_mashup), transpose=True)

        # mashupè°ƒç”¨tagçŸ©é˜µï¼ˆmashupè§†è§’ï¼‰
        mashup_tag_call_matrix = read_and_fill_matrix(mashup_tag_call_file, (self.n_mashup, self.n_tag), transpose=False)


        # APIè°ƒç”¨tagçŸ©é˜µï¼ˆAPIè§†è§’ï¼‰
        api_tag_call_matrix = read_and_fill_matrix(api_tag_call_file, (self.n_api, self.n_tag), transpose=False)

        # æ„é€ mashupè¶…è¾¹
        norm_mashup_call_1, norm_mashup_call_2 = build_and_normalize_hyperedge(mashup_api_call_matrix, alpha1, self.n_mashup,
                                                                               'mashup')
        # æ„é€ apiè¶…è¾¹
        norm_api_call_1, norm_api_call_2 = build_and_normalize_hyperedge(api_mashup_matrix, alpha2, self.n_api, 'api')

        # æ„é€ mashupè§†è§’çš„è¶…è¾¹ï¼Œmashupé—´è°ƒç”¨tagç›¸ä¼¼åº¦è¶…è¾¹
        norm_mashup_tag_1, norm_mashup_tag_2 = build_and_normalize_hyperedge(mashup_tag_call_matrix, alpha3,
                                                                                     self.n_mashup, 'mashupTagSim')
        # æ„é€ apiè§†è§’çš„è¶…è¾¹ï¼Œapié—´è°ƒç”¨tagç›¸ä¼¼åº¦è¶…è¾¹
        norm_api_tag_1, norm_api_tag_2 = build_and_normalize_hyperedge(api_tag_call_matrix, alpha4, self.n_api,
                                                                               'apiTagSim')

        return (
            norm_mashup_call_1, norm_mashup_call_2,
            norm_api_call_1, norm_api_call_2,
            norm_mashup_tag_1, norm_mashup_tag_2,
            norm_api_tag_1, norm_api_tag_2
        )


    def build_global_hyperedge(self, mashup_api_call_file, mashup_tag_call_file, api_tag_call_file,
                               N_MASHUP, N_API, N_TAG):

        """
        æ„é€ globalè§†è§’çš„è¶…è¾¹ï¼š
        - å¯¹æ¯ä¸ªmashupï¼Œæ„é€ ä¸€ä¸ªè¶…è¾¹ï¼šåŒ…å«mashupè‡ªèº«ã€å®ƒè°ƒç”¨çš„APIï¼ˆ0.8ï¼‰ã€å®ƒåŒ…å«çš„tagï¼ˆ0.5ï¼‰ã€‚
        - å¯¹æ¯ä¸ªAPIï¼Œæ„é€ ä¸€ä¸ªè¶…è¾¹ï¼šåŒ…å«APIè‡ªèº«ã€å®ƒä½¿ç”¨çš„tagï¼ˆ0.5ï¼‰ã€‚
        """

        def read_and_fill_matrix(filename, shape):
            matrix = np.zeros(shape)
            with open(filename) as f:
                for line in f:
                    line = line.strip()
                    if not line:
                        continue
                    parts = [item for item in line.split(" ") if item != '']
                    main_id = int(parts[0])
                    related_ids = list(map(int, parts[1:]))
                    matrix[main_id, related_ids] = 1
            return matrix

        # è¯»å–äº¤äº’çŸ©é˜µ
        mashup_api_matrix = read_and_fill_matrix(mashup_api_call_file, (N_MASHUP, N_API))
        mashup_tag_matrix = read_and_fill_matrix(mashup_tag_call_file, (N_MASHUP, N_TAG))
        api_tag_matrix = read_and_fill_matrix(api_tag_call_file, (N_API, N_TAG))

        hyperedge_rows = []
        hyperedge_cols = []
        hyperedge_values = []

        hyperedge_idx = 0

        # ---- å¯¹æ¯ä¸ª mashup æ„é€ è¶…è¾¹ ----
        for mashup in range(N_MASHUP):
            apis = np.where(mashup_api_matrix[mashup] > 0)[0]
            tags = np.where(mashup_tag_matrix[mashup] > 0)[0]

            hyperedge_rows.append(mashup)
            hyperedge_cols.append(hyperedge_idx)
            hyperedge_values.append(1.0)

            for api in apis:
                global_api_id = N_MASHUP + api
                hyperedge_rows.append(global_api_id)
                hyperedge_cols.append(hyperedge_idx)
                hyperedge_values.append(1)

            for tag in tags:
                global_tag_id = N_MASHUP + N_API + tag
                hyperedge_rows.append(global_tag_id)
                hyperedge_cols.append(hyperedge_idx)
                hyperedge_values.append(1)

            hyperedge_idx += 1

        # ---- å¯¹æ¯ä¸ª API æ„é€ è¶…è¾¹ ----
        for api in range(N_API):
            tags = np.where(api_tag_matrix[api] > 0)[0]

            global_api_id = N_MASHUP + api
            hyperedge_rows.append(global_api_id)
            hyperedge_cols.append(hyperedge_idx)
            hyperedge_values.append(1.0)

            for tag in tags:
                global_tag_id = N_MASHUP + N_API + tag
                hyperedge_rows.append(global_tag_id)
                hyperedge_cols.append(hyperedge_idx)
                hyperedge_values.append(1.0)

            hyperedge_idx += 1

        N_total = N_MASHUP + N_API + N_TAG
        N_edges = hyperedge_idx

        hyperedge_matrix = sp.coo_matrix(
            (hyperedge_values, (hyperedge_rows, hyperedge_cols)),
            shape=(N_total, N_edges)
        )

        hyperedge_T = hyperedge_matrix.T
        Dv, De = self.get_D_inv(hyperedge_T)

        Dv_torch = self.sparse_mx_to_torch_sparse_tensor(Dv)
        De_torch = self.sparse_mx_to_torch_sparse_tensor(De)
        hyperedge_torch = self.sparse_mx_to_torch_sparse_tensor(hyperedge_T)
        hyperedge_T_torch = self.sparse_mx_to_torch_sparse_tensor(hyperedge_matrix)

        temp = sparse.mm(Dv_torch, hyperedge_torch)
        normalized_1 = sparse.mm(temp, De_torch)
        normalized_2 = hyperedge_T_torch

        setattr(self, 'HGlobalEdge', hyperedge_torch)
        setattr(self, 'HGlobalEdge_T', hyperedge_T_torch)
        setattr(self, 'norm_global1', normalized_1)
        setattr(self, 'norm_global2', normalized_2)

        print(f"Number of global hyperedges: {N_edges}")

        return normalized_1, normalized_2

    def load_tag_labels(self, file_path, num_entity, num_tags):
        """
        Args:
            file_path: mashup_tags.csv æˆ– api_tags.csv
            num_entity: N_MASHUP æˆ– N_API
            num_tags: N_TAG
        Returns:
            labels: FloatTensor shape [num_entity, num_tags], 0/1è¡¨ç¤ºè¯¥entityæ˜¯å¦åŒ…å«è¯¥tag
        """
        labels = np.zeros((num_entity, num_tags), dtype=np.float32)
        with open(file_path, "r") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                parts = [item for item in line.split(" ") if item != '']
                entity_id = int(parts[0])
                tag_ids = [int(t) for t in parts[1:]]
                for tag_id in tag_ids:
                    if 0 <= tag_id < num_tags:
                        labels[entity_id, tag_id] = 1.0
        return torch.FloatTensor(labels)

    def set_lr_scheduler(self):  # lr_schedulerï¼šå­¦ä¹ ç‡è°ƒåº¦å™¨
        fac = lamb
        scheduler = optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda=fac)
        # æ¯æ¬¡çš„lrå€¼ï¼šæ¥è‡ªä¼˜åŒ–å™¨çš„åˆå§‹lrä¹˜ä¸Šä¸€ä¸ªlambda
        return scheduler

    def save_model(self):
        ensureDir(self.weights_save_path)
        torch.save(self.model.state_dict(), self.weights_save_path)

    def load_model(self):
        self.model.load_state_dict(torch.load(self.weights_save_path))


    def train(self):
        n_batch = data_generator.n_train // args.batch_size + 1
        patience = args.patience  # å®¹å¿å¤šå°‘ä¸ª epoch æ²¡æå‡
        no_improve_count = 0
        best_loss = float('inf')  # åˆå§‹åŒ–æœ€ä¼˜ loss
        best_epoch = -1  # è®°å½•æœ€ä½³ epoch
        for epoch in range(args.epoch):
            t1 = time()

            loss, mf_loss, emb_loss, reg_loss = 0., 0., 0., 0.
            mashup_cl_loss, api_cl_loss = 0.0, 0.0
            mashup_global_cl_loss, api_global_cl_loss = 0.0, 0.0
            n_batch = data_generator.n_train // args.batch_size + 1
            f_time, b_time, loss_time, opt_time, clip_time, emb_time = 0., 0., 0., 0., 0., 0.
            sample_time = 0.

            for idx in range(n_batch):
                self.model.train()  # æ¨¡å‹ä¸ºè®­ç»ƒæ¨¡å¼ï¼ŒåƒDropoutï¼ŒNormalizeè¿™äº›å±‚å°±ä¼šèµ·ä½œç”¨ï¼Œæµ‹è¯•æ¨¡å¼ä¸ä¼š

                self.optimizer.zero_grad()
                sample_t1 = time()
                users, pos_items, neg_items = data_generator.sample()  # é‡‡æ ·æ­£ç›¸å…³ä¸è´Ÿç›¸å…³çš„ç‰©å“id
                sample_time += time() - sample_t1

                (m_embeddings, a_embeddings,
                 m_c_proj, m_t_proj, a_c_proj, a_t_proj,
                 m_local_proj, a_local_proj, m_global_proj, a_global_proj)   = (
                    self.model(self.norm_mashup_call_1, self.norm_mashup_call_2,
                               self.norm_api_call_1, self.norm_api_call_2,
                               self.norm_mashup_tag_1, self.norm_mashup_tag_2,
                               self.norm_api_tag_1, self.norm_api_tag_2, self.global_1, self.global_2))

                u_g_embeddings = m_embeddings[users]
                pos_i_g_embeddings = a_embeddings[pos_items]
                neg_i_g_embeddings = a_embeddings[neg_items]

                batch_mf_loss, batch_emb_loss, batch_reg_loss = self.bpr_loss(u_g_embeddings, pos_i_g_embeddings,
                                                                              neg_i_g_embeddings)


                # contrastive loss for mashup: call vs tag
                batch_mashup_cl_loss = self.contrastive_loss(m_c_proj, m_t_proj, temperature=0.5)

                # contrastive loss for api: call vs tag
                batch_api_cl_loss = self.contrastive_loss(a_c_proj, a_t_proj, temperature=0.5)

                # contrastive loss for mashup: local vs global
                batch_mashup_global_cl_loss = self.contrastive_loss(m_local_proj, m_global_proj, temperature=0.5)

                # contrastive loss for api: local vs global
                batch_api_global_cl_loss = self.contrastive_loss(a_local_proj, a_global_proj, temperature=0.5)

                # å¯é€‰åŠ æƒ
                cl_loss_weight = args.beta  # å¯ä»¥é€šè¿‡ args ä¼ å…¥æ›´çµæ´»

                batch_loss = (batch_mf_loss + batch_emb_loss + batch_reg_loss)
                batch_loss += cl_loss_weight * batch_mashup_cl_loss + cl_loss_weight * batch_api_cl_loss
                batch_loss += cl_loss_weight * batch_mashup_global_cl_loss + cl_loss_weight * batch_api_global_cl_loss



                batch_loss.backward()
                self.optimizer.step()

                loss += float(batch_loss)
                mf_loss += float(batch_mf_loss)
                emb_loss += float(batch_emb_loss)
                reg_loss += float(batch_reg_loss)


                mashup_cl_loss += float(batch_mashup_cl_loss)
                api_cl_loss += float(batch_api_cl_loss)

                mashup_global_cl_loss += float(batch_mashup_global_cl_loss)
                api_global_cl_loss += float(batch_api_global_cl_loss)

            if epoch > 150:
                avg_loss = loss / n_batch
                # === ğŸ”¹ Early stopping based on loss ===
                if avg_loss < best_loss:
                    best_loss = avg_loss
                    no_improve_count = 0
                    best_epoch = epoch
                    if args.save_flag == 1:
                        self.save_model()  # ä¿å­˜å½“å‰æœ€ä¼˜æ¨¡å‹
                else:
                    no_improve_count += 1

                if no_improve_count >= patience:
                    print(
                        f"â¹ï¸ Early stopping triggered at epoch {epoch}. Best loss = {best_loss:.5f} (epoch {best_epoch})")
                    break

            self.lr_scheduler.step()  # å­¦ä¹ ç‡æ›´æ–°

            # print the test evaluation metrics each 10 epochs; pos:neg = 1:10.
            if (epoch + 1) % 10 != 0:
                if args.verbose > 0 and epoch % args.verbose == 0:
                    perf_str = (
                        f"Epoch {epoch} [{time() - t1:.1f}s]: "
                        f"loss={loss:.5f}, mf_loss={mf_loss:.5f}, "
                        f"mashup_cl_loss={mashup_cl_loss:.5f}, api_cl_loss={api_cl_loss:.5f}, "
                        f"mashup_global_cl_loss={mashup_global_cl_loss:.5f}, api_global_cl_loss={api_global_cl_loss:.5f}, "
                    )
                    print(perf_str)

            if math.isnan(loss) == True:
                print('ERROR: loss is nan.')
                sys.exit()




        # *********************************************************
        # save the user & item embeddings for pretraining.
        if args.save_flag == 1:
            self.save_model()
            print('save the weights in path: ', self.weights_save_path)




    def norm(self, adj):
        rowsum = np.array(adj.sum(1))
        d_inv = np.power(rowsum, -1).flatten()
        d_inv[np.isinf(d_inv)] = 0.
        d_mat_inv = sp.diags(d_inv)
        norm_adj = d_mat_inv.dot(adj)
        return norm_adj.tocoo()


    def cosine_similarity_scipy(self, vec1, vec2):
        """
        è®¡ç®—ä¸¤ä¸ªå‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦

        å‚æ•°:
        vec1 -- ç¬¬ä¸€ä¸ªå‘é‡ï¼ŒNumPy æ•°ç»„
        vec2 -- ç¬¬äºŒä¸ªå‘é‡ï¼ŒNumPy æ•°ç»„

        è¿”å›å€¼:
        ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œæµ®ç‚¹æ•°
        """
        return 1 - cosine(vec1, vec2)

    def get_vector_by_id(self, id):
        with h5py.File('data/vectors.h5', 'r') as f:
            if str(id) in f:
                return f[str(id)][:]
            else:
                return None

    def weighted_average(self, u_embeddings, similar):
        # å°† similar è½¬æ¢ä¸º PyTorch å¼ é‡
        similar = torch.tensor(similar, dtype=torch.float32).cuda()

        # è®¡ç®—æƒé‡çš„æ€»å’Œ
        total_weight = torch.sum(similar)

        # å°†æ¯ä¸ªå‘é‡ä¹˜ä»¥å…¶å¯¹åº”çš„æƒé‡
        weighted_embeddings = u_embeddings * similar.unsqueeze(1)

        # å¯¹æ‰€æœ‰åŠ æƒåçš„å‘é‡æ±‚å’Œ
        weighted_sum = torch.sum(weighted_embeddings, dim=0)

        # å°†åŠ æƒå’Œé™¤ä»¥æƒé‡çš„æ€»å’Œ
        u_avg = weighted_sum / total_weight

        return u_avg


    def print_final_results(self, rec_loger, pre_loger, ndcg_loger, hit_loger, map_loger, mrr_loger, fone_loger,
                            training_time_list):
        recs = np.array(rec_loger)
        pres = np.array(pre_loger)
        map = np.array(map_loger)
        mrr = np.array(mrr_loger)
        fone = np.array(fone_loger)

        best_rec_0 = max(recs[:, 0])
        idx = list(recs[:, 0]).index(best_rec_0)

        final_perf = "Best Iter=[%d]@[%.1f]\trecall=[%s], precision=[%s], hit=[%s], ndcg=[%s], map=[%s],mrr=[%s], f1=[%s]" % \
                     (idx, time() - t0, '\t'.join(['%.5f' % r for r in recs[idx]]),
                      '\t'.join(['%.5f' % r for r in pres[idx]]),
                      '\t'.join(['%.5f' % r for r in hit_loger[idx]]),
                      '\t'.join(['%.5f' % r for r in ndcg_loger[idx]]),
                      '\t'.join(['%.5f' % r for r in map[idx]]),
                      '\t'.join(['%.5f' % r for r in mrr[idx]]),
                      '\t'.join(['%.5f' % r for r in fone[idx]]))
        result.append(final_perf + "\n")
        print(final_perf)

    # pos_itemsï¼šæ­£ç›¸å…³ç‰©å“çš„id
    # neg_itemsï¼šè´Ÿç›¸å…³ç‰©å“çš„id
    def bpr_loss(self, users, pos_items, neg_items):
        pos_scores = torch.sum(torch.mul(users, pos_items), dim=1)  # torch.mul():å¯¹åº”å…ƒç´ ç›¸ä¹˜
        neg_scores = torch.sum(torch.mul(users, neg_items), dim=1)  # torch.mul():å¯¹åº”å…ƒç´ ç›¸ä¹˜

        regularizer = 1. / 2 * (users ** 2).sum() + 1. / 2 * (pos_items ** 2).sum() + 1. / 2 * (neg_items ** 2).sum()
        regularizer = regularizer / self.batch_size

        maxi = F.logsigmoid(pos_scores - neg_scores)
        mf_loss = -torch.mean(maxi)

        emb_loss = self.decay * regularizer
        reg_loss = 0.0
        return mf_loss, emb_loss, reg_loss



    def contrastive_loss(self, z1, z2, temperature=0.5):
        # ä¿ç•™åŸå§‹å‘é‡ï¼Œä¸å½’ä¸€åŒ–
        N = z1.size(0)

        # æ­£æ ·æœ¬è·ç¦»
        pos_dist = torch.norm(z1 - z2, p=2, dim=1)  # [N]
        sim_pos = -pos_dist / temperature

        # è®¡ç®— pairwise è·ç¦»çš„å¹³æ–¹ï¼ˆé«˜æ•ˆï¼‰
        z1_sq = (z1 ** 2).sum(dim=1).unsqueeze(1)  # [N, 1]
        z2_sq = (z2 ** 2).sum(dim=1).unsqueeze(0)  # [1, N]
        dot_product = torch.matmul(z1, z2.T)  # [N, N]

        # æ¬§å‡ é‡Œå¾—è·ç¦»å¹³æ–¹
        dist_squared = z1_sq + z2_sq - 2 * dot_product
        dist_squared = torch.clamp(dist_squared, min=1e-8)  # é¿å…è´Ÿæ•°è¯¯å·®

        # å–å¹³æ–¹æ ¹ï¼Œå¾—åˆ°è·ç¦»çŸ©é˜µ
        dist_matrix = torch.sqrt(dist_squared)  # [N, N]

        # è½¬ä¸ºè´Ÿç›¸ä¼¼åº¦
        logits_zw = -dist_matrix / temperature

        # å±è”½æ­£æ ·æœ¬
        mask = torch.eye(N, dtype=torch.bool).to(z1.device)
        logits_zw.masked_fill_(mask, -1e9)

        # åˆ†æ¯ï¼šæ‰€æœ‰è´Ÿæ ·æœ¬
        denom = torch.exp(logits_zw).sum(dim=1)

        # æœ€ç»ˆæŸå¤±
        loss = - torch.log(torch.exp(sim_pos) / (torch.exp(sim_pos) + denom + 1e-8))
        return loss.mean()

    def sparse_mx_to_torch_sparse_tensor(self, sparse_mx):
        """Convert a scipy sparse matrix to a torch sparse tensor."""
        sparse_mx = sparse_mx.tocoo().astype(np.float32)
        indices = torch.from_numpy(
            np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))
        values = torch.from_numpy(sparse_mx.data)
        shape = torch.Size(sparse_mx.shape)
        return torch.sparse.FloatTensor(indices, values, shape)

    def get_sparse_tensor_value(self, X):
        coo = X.tocoo().astype(np.float32)
        indices = np.mat([coo.row, coo.col]).transpose()
        return indices, coo.data, coo.shape


def main():
    # è°ƒç”¨ parse_args è·å–å‘½ä»¤è¡Œå‚æ•°
    # args = parse_args()
    # args.set_defaults(dataset='fold_2')
    os.environ["CUDA_VISIBLE_DEVICES"] = str(args.gpu_id)

    config = dict()
    config['n_users'] = data_generator.n_users
    config['n_items'] = data_generator.n_items
    config['n_tags'] = data_generator.n_tags

    t0 = time()

    pretrain_data = None

    Engine = Model_Wrapper(data_config=config, pretrain_data=pretrain_data)
    if args.pretrain:
        print('pretrain path: ', Engine.weights_save_path)
        if os.path.exists(Engine.weights_save_path):
            Engine.load_model()
            Engine.train()
        else:
            print('Cannot load pretrained model. Start training from stratch')
    else:
        print('without pretraining')
        Engine.train()


if __name__ == '__main__':
    main()